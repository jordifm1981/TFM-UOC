{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate cnn lstm example\n",
    "from numpy import array\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def train_test (n_steps_in, n_steps_out, propTrainTest, dataset):\n",
    "    \n",
    "    n_steps_in, n_steps_out = n_steps_in, n_steps_out\n",
    "    # split train test\n",
    "    X, y = split_sequence(dataset, n_steps_in, n_steps_out)\n",
    "    \n",
    "    train_size = int(len(X) * propTrainTest)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_X, test_X = X[0:train_size,:], X[train_size:len(X),:]\n",
    "    train_Y, test_Y = y[0:train_size,:], y[train_size:len(y),:]\n",
    "    \n",
    "    return (train_X, test_X, train_Y, test_Y)\n",
    "\n",
    "# fixem random seed\n",
    "numpy.random.seed(7)\n",
    "\n",
    "dadesSau = read_csv('dadesBaells.csv', sep=';',header=0, index_col=0)\n",
    "dataframe = pd.DataFrame(dadesSau.loc[dadesSau.index >= '2000-01-01']['Volum'])\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# Normalitzem\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dataset = [item for sublist in dataset for item in sublist]\n",
    "\n",
    "n_steps_in = 20\n",
    "n_steps_out = 15\n",
    "propTrainTest = 0.8\n",
    "train_X, test_X, train_Y, test_Y = train_test(n_steps_in, n_steps_out, propTrainTest, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = np.arange(10,501,20).tolist()\n",
    "# Number of features to consider at every split\n",
    "max_features = [1,5,6,7,10,12,15,20]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = np.arange(1,121,5).tolist()\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = np.arange(2,50,3)\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = np.arange(1,41,5).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=500, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330, 350, 370, 390, 410, 430, 450, 470, 490], 'max_features': [1, 5, 6, 7, 10, 12, 15, 20], 'max_depth': [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116], 'min_samples_split': array([ 2,  5,  8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47]), 'min_samples_leaf': [1, 6, 11, 16, 21, 26, 31, 36]},\n",
       "          pre_dispatch='2*n_jobs', random_state=125, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 500, cv=TimeSeriesSplit(n_splits=5).get_n_splits([train_X,train_Y]), scoring='neg_mean_squared_error', random_state = 125, n_jobs=-1, verbose=0)\n",
    "\n",
    "rf_random.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 490, 'min_samples_split': 14, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 101}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 250, 'min_samples_split': 14, 'min_samples_leaf': 16, 'max_features': 20, 'max_depth': 111}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 470, 'min_samples_split': 23, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 96}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 150, 'min_samples_split': 23, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 66}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 290, 'min_samples_split': 23, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 41}\n",
      "\n",
      "Model with rank: 6\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 390, 'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 101}\n",
      "\n",
      "Model with rank: 7\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 470, 'min_samples_split': 41, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 26}\n",
      "\n",
      "Model with rank: 8\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 30, 'min_samples_split': 5, 'min_samples_leaf': 16, 'max_features': 20, 'max_depth': 6}\n",
      "\n",
      "Model with rank: 9\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 20, 'max_depth': 66}\n",
      "\n",
      "Model with rank: 10\n",
      "Mean validation score: -0.001 (std: 0.000)\n",
      "Parameters: {'n_estimators': 210, 'min_samples_split': 44, 'min_samples_leaf': 16, 'max_features': 20, 'max_depth': 46}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=10):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "report(rf_random.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplement aplicarem el mètode amb els paràmetres òptims obtinguts de l'apartat anterior\n",
    "modelRFOpt = RandomForestRegressor(n_estimators= 490, min_samples_split= 14, min_samples_leaf= 11, \n",
    "                              max_features = 20, max_depth= 101)\n",
    "modelRFOpt.fit(train_X, train_Y)\n",
    "\n",
    "# predicció amb dades de test\n",
    "testPredict = modelRFOpt.predict(test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRFOpt2 = RandomForestRegressor(n_estimators= 170, min_weight_fraction_leaf= 0, min_samples_split= 67, min_samples_leaf= 10, \n",
    "                                   max_leaf_nodes= 54, max_features= None, max_depth= 71, bootstrap = True)\n",
    "modelRFOpt2.fit(train_X, train_Y)\n",
    "testPredict2 = modelRFOpt2.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRFOpt3 = RandomForestRegressor(n_estimators= 70, min_weight_fraction_leaf= 0, min_samples_split= 77, min_samples_leaf= 25,\n",
    "                                    max_leaf_nodes= 52, max_features= 'auto', max_depth= 11, bootstrap= True)\n",
    "modelRFOpt3.fit(train_X, train_Y)\n",
    "testPredict3 = modelRFOpt3.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9581023402140523\n"
     ]
    }
   ],
   "source": [
    "test_YR = scaler.inverse_transform(test_Y)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculem RMSE per cada dia\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculem MSE\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculem RMSE\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calcul global de RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n",
    "\n",
    "RMSE_TOT, RMSE_days = evaluate_forecasts(test_YR,testPredict)\n",
    "print(RMSE_TOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'bootstrap': [True], 'max_depth': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 'max_features': ['auto'], 'min_samples_leaf': [25], 'min_samples_split': [77], 'n_estimators': [50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88], 'max_leaf_nodes': [52]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = np.arange(50,90,2).tolist()\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = np.arange(5,25,1).tolist()\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [77] #np.arange(70,85,2)\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [25] #np.arange(20,31,4).tolist()\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "max_leaf_nodes = [52]#np.arange(45,60,1).tolist()\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': bootstrap,\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_leaf_nodes': max_leaf_nodes\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv=TimeSeriesSplit(n_splits=5).get_n_splits([train_X,train_Y]), scoring='neg_mean_squared_error', n_jobs = -1, verbose = 0)\n",
    "\n",
    "grid_search.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': 52,\n",
       " 'min_samples_leaf': 25,\n",
       " 'min_samples_split': 77,\n",
       " 'n_estimators': 86}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility function to report best scores\n",
    "def grid_scores_to_df(grid_scores):\n",
    "    \"\"\"\n",
    "    Convert a sklearn.grid_search.GridSearchCV.grid_scores_ attribute to a tidy\n",
    "    pandas DataFrame where each row is a hyperparameter-fold combinatination.\n",
    "    \"\"\"\n",
    "    rows = list()\n",
    "    for grid_score in grid_scores:\n",
    "        for fold, score in enumerate(grid_score.cv_validation_scores):\n",
    "            row = grid_score.parameters.copy()\n",
    "            row['fold'] = fold\n",
    "            row['score'] = score\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "#grid_scores_to_df(grid_search.grid_scores_)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRFOpt4 = RandomForestRegressor(n_estimators= 86, min_weight_fraction_leaf= 0, min_samples_split= 77, min_samples_leaf= 25, \n",
    "                                   max_leaf_nodes= 52, max_features= 'auto', max_depth=5, bootstrap = True)\n",
    "modelRFOpt4.fit(train_X, train_Y)\n",
    "testPredict4 = modelRFOpt4.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.23505358607499\n"
     ]
    }
   ],
   "source": [
    "test_YR4 = scaler.inverse_transform(test_Y)\n",
    "testPredict4 = scaler.inverse_transform(testPredict4)\n",
    "\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculem RMSE per cada dia\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculem MSE\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculem RMSE\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    # calcul global de RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores\n",
    "\n",
    "RMSE_TOT, RMSE_days = evaluate_forecasts(test_YR4,testPredict4)\n",
    "print(RMSE_TOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════╤═════════╤═════════╤═════════╕\n",
      "│    dia 1 │   dia 2 │   dia 3 │   dia 4 │   dia 5 │\n",
      "╞══════════╪═════════╪═════════╪═════════╪═════════╡\n",
      "│ 0.899697 │ 1.21273 │  1.5334 │  1.8448 │ 2.14941 │\n",
      "╘══════════╧═════════╧═════════╧═════════╧═════════╛\n",
      "╒═════════╤═════════╤═════════╤═════════╤══════════╕\n",
      "│   dia 6 │   dia 7 │   dia 8 │   dia 9 │   dia 10 │\n",
      "╞═════════╪═════════╪═════════╪═════════╪══════════╡\n",
      "│ 2.44922 │ 2.74741 │ 3.04331 │ 3.33897 │  3.62501 │\n",
      "╘═════════╧═════════╧═════════╧═════════╧══════════╛\n",
      "╒══════════╤══════════╤══════════╤══════════╤══════════╤═════════╕\n",
      "│   dia 11 │   dia 12 │   dia 13 │   dia 14 │   dia 15 │   Total │\n",
      "╞══════════╪══════════╪══════════╪══════════╪══════════╪═════════╡\n",
      "│  3.89586 │  4.15215 │  4.40075 │   4.6455 │  4.88523 │ 3.23505 │\n",
      "╘══════════╧══════════╧══════════╧══════════╧══════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "headers=[]\n",
    "for i in range(len(RMSE_days)):\n",
    "    headers.append('dia '+str(i+1))\n",
    "\n",
    "headers.append('Total')\n",
    "v = RMSE_days\n",
    "ultim = v.append(RMSE_TOT)\n",
    "table1 = tabulate([RMSE_days[0:5]], headers[0:5], tablefmt=\"fancy_grid\")\n",
    "table2 = tabulate([RMSE_days[5:10]], headers[5:10], tablefmt=\"fancy_grid\")\n",
    "table3 = tabulate([RMSE_days[10:16]], headers[10:16], tablefmt=\"fancy_grid\")\n",
    "#output\n",
    "print(table1)\n",
    "print(table2)\n",
    "print(table3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
